{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(tokens, n):\n",
    "    replacements = {}  # Store replacement mappings\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Count occurrences of each pair of numbers in the list\n",
    "        pairs = [(tokens[j], tokens[j+1]) for j in range(len(tokens) - 1)]\n",
    "        pair_counts = Counter(pairs)\n",
    "        \n",
    "        # Find the most common pair\n",
    "        if not pair_counts:\n",
    "            print(\"No more pairs to process.\")\n",
    "            break\n",
    "        \n",
    "        most_common_pair, most_common_count = pair_counts.most_common(1)[0]\n",
    "        \n",
    "        # Determine the new code point\n",
    "        new_code_point = 255 + i + 1\n",
    "        \n",
    "        # Store the replacement mapping\n",
    "        replacements[new_code_point] = most_common_pair\n",
    "        \n",
    "        # Replace the most common pair with the new code point in tokens\n",
    "        new_tokens = []\n",
    "        skip = False\n",
    "        \n",
    "        for j in range(len(tokens) - 1):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            \n",
    "            if (tokens[j], tokens[j+1]) == most_common_pair:\n",
    "                new_tokens.append(new_code_point)\n",
    "                skip = True\n",
    "            else:\n",
    "                new_tokens.append(tokens[j])\n",
    "        \n",
    "        # Add the last token if it was not part of a pair\n",
    "        if not skip and len(tokens) > 0:\n",
    "            new_tokens.append(tokens[-1])\n",
    "        \n",
    "        tokens = new_tokens\n",
    "    \n",
    "    return replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, replacements):\n",
    "    # Convert the input string to a list of UTF-8 encoded bytes\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    \n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    # Apply each replacement in the order of the replacements dictionary\n",
    "    for code_point, pair in replacements.items():\n",
    "        encoded_tokens = []\n",
    "        skip = False\n",
    "        \n",
    "        for j in range(len(new_tokens) - 1):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            \n",
    "            if (new_tokens[j], new_tokens[j+1]) == pair:\n",
    "                encoded_tokens.append(code_point)\n",
    "                skip = True\n",
    "            else:\n",
    "                encoded_tokens.append(new_tokens[j])\n",
    "        \n",
    "        # Add the last token if it was not part of a pair\n",
    "        if not skip and len(new_tokens) > 0:\n",
    "            encoded_tokens.append(new_tokens[-1])\n",
    "        \n",
    "        new_tokens = encoded_tokens\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(tokens, replacements):\n",
    "    # Create a new list to store the decoded tokens\n",
    "    original_tokens = tokens.copy()  # Start with the encoded tokens\n",
    "\n",
    "    # Keep decoding until no further replacements are possible\n",
    "    while True:\n",
    "        # Track whether any replacements were made in this pass\n",
    "        replaced = False\n",
    "        new_tokens = []\n",
    "\n",
    "        i = 0\n",
    "        while i < len(original_tokens):\n",
    "            token = original_tokens[i]\n",
    "            \n",
    "            if token in replacements:\n",
    "                replaced = True\n",
    "                new_tokens.extend(replacements[token])  # Replace token with original pair\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "            i += 1\n",
    "        \n",
    "        # If no replacements were made, we are done\n",
    "        if not replaced:\n",
    "            break\n",
    "        \n",
    "        # Update original_tokens with the new_tokens for the next iteration\n",
    "        original_tokens = new_tokens\n",
    "\n",
    "    # Ensure all values are within the valid byte range\n",
    "    original_tokens = [t for t in original_tokens if 0 <= t <= 255]\n",
    "\n",
    "    # Reconstruct the original bytes from the original tokens\n",
    "    try:\n",
    "        original_bytes = bytes(original_tokens)\n",
    "        original_string = original_bytes.decode('utf-8', errors='replace')\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError: {e}\")\n",
    "        original_string = None\n",
    "\n",
    "    return original_tokens, original_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacements Mapping: {256: (32, 97), 257: (101, 32), 258: (257, 105)}\n",
      "Encoded Tokens: [72, 105, 32, 109, 121, 32, 110, 97, 109, 258, 115, 256, 121, 117, 115, 104, 256, 110, 100, 32, 105, 256, 109, 32, 116, 104, 257, 98, 101, 115, 116, 32, 112, 101, 114, 115, 111, 110, 256, 108, 105, 118, 258, 110, 32, 116, 104, 105, 115, 32, 119, 111, 114, 108, 100, 32, 119, 104, 111, 108, 101]\n",
      "Original Tokens: [72, 105, 32, 109, 121, 32, 110, 97, 109, 101, 32, 105, 115, 32, 97, 121, 117, 115, 104, 32, 97, 110, 100, 32, 105, 32, 97, 109, 32, 116, 104, 101, 32, 98, 101, 115, 116, 32, 112, 101, 114, 115, 111, 110, 32, 97, 108, 105, 118, 101, 32, 105, 110, 32, 116, 104, 105, 115, 32, 119, 111, 114, 108, 100, 32, 119, 104, 111, 108, 101]\n",
      "The original tokens were successfully reconstructed.\n",
      "Original String: Hi my name is ayush and i am the best person alive in this world whole\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi my name is ayush and i am the best person alive in this world whole\"\n",
    "tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "n = 3  # Number of iterations\n",
    "replacements = train(tokens, n)\n",
    "print(\"Replacements Mapping:\", replacements)\n",
    "\n",
    "# Encode the text using the generated replacements\n",
    "encoded_tokens = encode(text, replacements)\n",
    "print(\"Encoded Tokens:\", encoded_tokens)\n",
    "\n",
    "# Decode the tokens\n",
    "original_tokens, original_string = decode_tokens(encoded_tokens, replacements)\n",
    "print(\"Original Tokens:\", original_tokens)\n",
    "if original_tokens == tokens:\n",
    "    print(\"The original tokens were successfully reconstructed.\")\n",
    "else:\n",
    "    print(\"The original tokens were not successfully reconstructed.\")\n",
    "print(\"Original String:\", original_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
